{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outer clothing': 0.55, 'on': 0.41388888888888886, 'under': 0.4111111111111111}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データ読み込み\n",
    "data_path = 'C:/1作品/clothse_excel/train1800.xlsx'\n",
    "data = pd.read_excel(data_path)\n",
    "\n",
    "# 前処理\n",
    "data_cleaned = data.drop(['Unnamed: 0', 'date', 'name'], axis=1)\n",
    "\n",
    "# カテゴリ特徴量をエンコード\n",
    "label_encoders = {}\n",
    "for column in ['season']:\n",
    "    le = LabelEncoder()\n",
    "    data_cleaned[column] = le.fit_transform(data_cleaned[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# 説明変数と目的変数分け\n",
    "X = data_cleaned.drop(['outer clothing', 'on', 'under'], axis=1)\n",
    "y = data_cleaned[['outer clothing', 'on', 'under']]\n",
    "\n",
    "# データセットを分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# モデル訓練\n",
    "model_classifier = RandomForestClassifier(n_estimators=100,min_samples_split=5, max_depth=None, random_state=42)\n",
    "model_classifier.fit(X_train, y_train)\n",
    "\n",
    "model_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 予測\n",
    "y_pred_class = model_classifier.predict(X_test)\n",
    "\n",
    "#目的変数の精度\n",
    "accuracies = [accuracy_score(y_test.iloc[:, i], y_pred_class[:, i]) for i in range(y_test.shape[1])]\n",
    "\n",
    "accuracies_dict = {\n",
    "    \"outer clothing\": accuracies[0],\n",
    "    \"on\": accuracies[1],\n",
    "    \"under\": accuracies[2]\n",
    "}\n",
    "\n",
    "\n",
    "print(accuracies_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   5.9s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   1.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=None, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.2s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.0s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=10, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=50; total time=   0.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=2, estimator__n_estimators=100; total time=   0.4s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=5, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=10; total time=   0.0s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.2s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=50; total time=   0.1s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "[CV] END estimator__max_depth=20, estimator__min_samples_split=10, estimator__n_estimators=100; total time=   0.3s\n",
      "Best parameters: {'estimator__max_depth': None, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 100}\n",
      "Best score: 0.8580000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# 创建一个示例多标签（多输出）数据集\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=20, n_classes=3, n_labels=2, random_state=42)\n",
    "\n",
    "# 定义基础模型 - 随机森林分类器\n",
    "base_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 创建多输出分类器，使用随机森林作为基础模型\n",
    "multi_output_rf = MultiOutputClassifier(base_rf, n_jobs=-1)\n",
    "\n",
    "# 定义参数网格，注意参数名前加 'estimator__' 是因为我们是在调整包装器内部的估计器的参数\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [10, 50, 100],  # 树的数量\n",
    "    'estimator__max_depth': [None, 10, 20],  # 树的最大深度\n",
    "    'estimator__min_samples_split': [2, 5, 10]  # 分裂内部节点所需的最小样本数\n",
    "}\n",
    "\n",
    "# 定义评分函数，这里我们计算所有目标的平均准确率\n",
    "def multioutput_accuracy(y_true, y_pred):\n",
    "    # 计算所有输出的平均准确率\n",
    "    scores = [accuracy_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])]\n",
    "    return np.mean(scores)\n",
    "\n",
    "# 创建一个make_scorer对象\n",
    "scorer = make_scorer(multioutput_accuracy)\n",
    "\n",
    "# 设置网格搜索，使用定义好的评分函数\n",
    "grid_search = GridSearchCV(estimator=multi_output_rf, param_grid=param_grid, scoring=scorer, cv=5, verbose=2)\n",
    "\n",
    "# 拟合网格搜索\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 输出最佳参数和对应的评分\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_tem</th>\n",
       "      <th>min_tem</th>\n",
       "      <th>mean_tem</th>\n",
       "      <th>average_humidity</th>\n",
       "      <th>average_wind_speed(m/s)</th>\n",
       "      <th>sensible_temperature</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.35</td>\n",
       "      <td>31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.35</td>\n",
       "      <td>31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.35</td>\n",
       "      <td>31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.35</td>\n",
       "      <td>31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.35</td>\n",
       "      <td>31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>23.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.543294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>23.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.543294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>23.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.543294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>23.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.543294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>23.3</td>\n",
       "      <td>10.3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>63</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.543294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1799 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_tem  min_tem  mean_tem  average_humidity  average_wind_speed(m/s)  \\\n",
       "0         6.8      3.9      5.35                31                      2.3   \n",
       "1         6.8      3.9      5.35                31                      2.3   \n",
       "2         6.8      3.9      5.35                31                      2.3   \n",
       "3         6.8      3.9      5.35                31                      2.3   \n",
       "4         6.8      3.9      5.35                31                      2.3   \n",
       "...       ...      ...       ...               ...                      ...   \n",
       "1794     23.3     10.3     16.80                63                      2.6   \n",
       "1795     23.3     10.3     16.80                63                      2.6   \n",
       "1796     23.3     10.3     16.80                63                      2.6   \n",
       "1797     23.3     10.3     16.80                63                      2.6   \n",
       "1798     23.3     10.3     16.80                63                      2.6   \n",
       "\n",
       "      sensible_temperature  season  \n",
       "0                 0.733549       3  \n",
       "1                 0.733549       3  \n",
       "2                 0.733549       3  \n",
       "3                 0.733549       3  \n",
       "4                 0.733549       3  \n",
       "...                    ...     ...  \n",
       "1794             17.543294       1  \n",
       "1795             17.543294       1  \n",
       "1796             17.543294       1  \n",
       "1797             17.543294       1  \n",
       "1798             17.543294       1  \n",
       "\n",
       "[1799 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/1作品/picture_myclothes/season_encoder.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n",
    "\n",
    "# # 保存标签编码器\n",
    "# joblib.dump(label_encoders['season'], 'C:/1作品/picture_myclothes/season_encoder.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 定义Excel文件路径\n",
    "excel_filename = \"C:/1作品/clothse_excel/classification_report1.xlsx\"\n",
    "\n",
    "# 类别名称列表定义\n",
    "category_names = {\n",
    "    'outer clothing': ['Hoodie', 'Unknown', 'coat', 'denim jacket', 'jackets', 'knitted coat', 'long down jacket', 'shirts', 'short down jacket', 'short trench coat', 'short woolen coat', 'shorts', 'suit jackets', 'sweater', 't-shirts', 'windbreaker'],\n",
    "    'on': ['Hoodie', 'Unknown', 'base layer', 'denim jacket', 'long-sleeved dress', 'shirts', 'suit jackets', 'sweater', 't-shirts', 'turtleneck sweater', 'vest'],\n",
    "    'under': ['dress', 'jeans', 'knitted pants', 'leather pants', 'long skirt', 'long-sleeved dress', 'pants', 'short pants', 'short skirt', 'sportswear', 'suit pants', 'woolen pants']\n",
    "}\n",
    "\n",
    "# 初始化Excel写入器\n",
    "with pd.ExcelWriter(excel_filename) as writer:\n",
    "    for i, target in enumerate(['outer clothing', 'on', 'under']):\n",
    "        # 生成分类报告和混淆矩阵（这里使用示例数据，你需要替换为实际的y_test和y_pred_class）\n",
    "        # report = classification_report(y_test.iloc[:, i], y_pred_class[:, i], zero_division=0)\n",
    "        cm = confusion_matrix(y_test.iloc[:, i], y_pred_class[:, i])\n",
    "        \n",
    "        # 创建DataFrame\n",
    "        cm_df = pd.DataFrame(cm, index=category_names[target], columns=category_names[target])\n",
    "        \n",
    "        # 将DataFrame保存到Excel的不同工作表\n",
    "        cm_df.to_excel(writer, sheet_name=target)\n",
    "\n",
    "# 注意：无需显式调用writer.save()，因为with语句块结束时会自动保存和关闭文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # 定义Excel文件路径\n",
    "# excel_filename = \"C:/1作品/clothse_excel/classification_report_and_confusion_matrix.xlsx\"\n",
    "\n",
    "# # 类别名称列表定义\n",
    "# category_names = {\n",
    "#     'outer clothing': ['Hoodie', 'Unknown', 'coat', 'denim jacket', 'jackets', 'knitted coat', 'long down jacket', 'shirts', 'short down jacket', 'short trench coat', 'short woolen coat', 'shorts', 'suit jackets', 'sweater', 't-shirts', 'windbreaker'],\n",
    "#     'on': ['Hoodie', 'Unknown', 'base layer', 'denim jacket', 'long-sleeved dress', 'shirts', 'suit jackets', 'sweater', 't-shirts', 'turtleneck sweater', 'vest'],\n",
    "#     'under': ['dress', 'jeans', 'knitted pants', 'leather pants', 'long skirt', 'long-sleeved dress', 'pants', 'short pants', 'short skirt', 'sportswear', 'suit pants', 'woolen pants']\n",
    "# }\n",
    "\n",
    "# # 初始化Excel写入器\n",
    "# with pd.ExcelWriter(excel_filename) as writer:\n",
    "#     for i, target in enumerate(['outer clothing', 'on', 'under']):\n",
    "#         # 生成分类报告（这里使用示例数据，你需要替换为实际的y_test和y_pred_class）\n",
    "#         report = classification_report(y_test.iloc[:, i], y_pred_class[:, i], target_names=category_names[target], zero_division=0)\n",
    "        \n",
    "#         # 将分类报告转换为DataFrame\n",
    "#         report_df = pd.DataFrame([x.split() for x in report.split('\\n')])\n",
    "\n",
    "#         # 保存分类报告到Excel的不同工作表\n",
    "#         report_sheet_name = f\"{target}_report\"\n",
    "#         report_df.to_excel(writer, sheet_name=report_sheet_name, index=False, header=False)\n",
    "\n",
    "#         # 生成混淆矩阵\n",
    "#         cm = confusion_matrix(y_test.iloc[:, i], y_pred_class[:, i])\n",
    "        \n",
    "#         # 创建混淆矩阵的DataFrame\n",
    "#         cm_df = pd.DataFrame(cm, index=category_names[target], columns=category_names[target])\n",
    "        \n",
    "#         # 保存混淆矩阵到Excel的不同工作表\n",
    "#         cm_sheet_name = f\"{target}_confusion_matrix\"\n",
    "#         cm_df.to_excel(writer, sheet_name=cm_sheet_name)\n",
    "\n",
    "# # 注意：无需显式调用writer.save()，因为with语句块结束时会自动保存和关闭文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# report_filename = \"C:/1作品/clothse_excel/classification_report.txt\"\n",
    "\n",
    "\n",
    "# with open(report_filename, \"w\") as file:\n",
    "#     file.write(\"例:\\n\")\n",
    "#     np.savetxt(file, y_pred_class[:5], fmt='%s', header=' '.join(['outer clothing', 'on', 'under']))\n",
    "\n",
    "#     # 各目的変数の評価指標を計算\n",
    "#     for i, target in enumerate(['outer clothing', 'on', 'under']):\n",
    "#         file.write(f\"\\n{target} の評価:\\n\")\n",
    "#         report = classification_report(y_test.iloc[:, i], y_pred_class[:, i], zero_division=0)\n",
    "#         file.write(report)\n",
    "#         file.write(\"\\n混同行列:\\n\")\n",
    "#         cm = confusion_matrix(y_test.iloc[:, i], y_pred_class[:, i])\n",
    "#         cm_str = '\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in cm])\n",
    "#         file.write(cm_str + \"\\n\")\n",
    "\n",
    "\n",
    "# report_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# report_filename = \"C:/1作品/clothse_excel/classification_report1.txt\"\n",
    "\n",
    "# # 假设类别名称列表已经定义好，例如：\n",
    "# category_names = {\n",
    "#     'outer clothing': ['Hoodie', 'Unknown', 'coat', 'denim jacket', 'jackets', 'knitted coat', 'long down jacket', 'shirts', 'short down jacket', 'short trench coat', 'short woolen coat', 'shorts', 'suit jackets', 'sweater', 't-shirts', 'windbreaker'],\n",
    "#     'on': ['Hoodie', 'Unknown', 'base layer', 'denim jacket', 'long-sleeved dress', 'shirts', 'suit jackets', 'sweater', 't-shirts', 'turtleneck sweater', 'vest'],\n",
    "#     'under': ['dress', 'jeans', 'knitted pants', 'leather pants', 'long skirt', 'long-sleeved dress', 'pants', 'short pants', 'short skirt', 'sportswear', 'suit pants', 'woolen pants']\n",
    "# }\n",
    "\n",
    "# with open(report_filename, \"w\") as file:\n",
    "#     file.write(\"例:\\n\")\n",
    "#     # 此处假设y_pred_class[:5]已正确生成并包含预期格式的数据\n",
    "#     np.savetxt(file, y_pred_class[:5], fmt='%s', header=' '.join(['outer clothing', 'on', 'under']))\n",
    "\n",
    "#     for i, target in enumerate(['outer clothing', 'on', 'under']):\n",
    "#         file.write(f\"\\n{target} の評価:\\n\")\n",
    "#         report = classification_report(y_test.iloc[:, i], y_pred_class[:, i], zero_division=0)\n",
    "#         file.write(report)\n",
    "        \n",
    "#         file.write(\"\\n混同行列:\\n\")\n",
    "#         cm = confusion_matrix(y_test.iloc[:, i], y_pred_class[:, i])\n",
    "#         # 创建DataFrame，增加行列的可读性\n",
    "#         cm_df = pd.DataFrame(cm, index=category_names[target], columns=category_names[target])\n",
    "#         # 将DataFrame转换为字符串并写入文件\n",
    "#         cm_str = cm_df.to_string()\n",
    "#         file.write(cm_str + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存至 C:/1作品/clothse_excel/random_forest_classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from joblib import dump\n",
    "\n",
    "# # 假设您已经完成了数据预处理并分割了数据集\n",
    "\n",
    "# # 定义模型\n",
    "# model_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # 训练模型\n",
    "# model_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # 指定模型保存路径\n",
    "# model_path = 'C:/1作品/clothse_excel/random_forest_classifier.joblib'\n",
    "\n",
    "# # 保存模型\n",
    "# dump(model_classifier, model_path)\n",
    "\n",
    "# print(f\"模型已保存至 {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
