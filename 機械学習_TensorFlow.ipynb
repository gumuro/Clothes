{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koyama\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# 假设您已经加载了数据到data_full\n",
    "data_full = pd.read_excel('C:/1作品/clothse_excel/train1800.xlsx')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "# 对分类特征进行编码\n",
    "label_encoder = LabelEncoder()\n",
    "data_full['wether'] = label_encoder.fit_transform(data_full['wether'])\n",
    "data_full['season'] = label_encoder.fit_transform(data_full['season'])\n",
    "\n",
    "\n",
    "features = data_full[['max_tem', 'min_tem', 'mean_tem', 'average_humidity', 'average_wind_speed(m/s)', 'sensible_temperature','season','wether']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "labels = data_full[['label1', 'label2', 'label3']]\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_standardized, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(input_shape, output_units, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(output_units, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-13 12:42:17,528] A new study created in memory with name: no-name-300e5f91-0e33-4131-82da-5d72ac82513b\n",
      "C:\\Users\\koyama\\AppData\\Local\\Temp\\ipykernel_5992\\3250495418.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "C:\\Users\\koyama\\AppData\\Local\\Temp\\ipykernel_5992\\3250495418.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "[I 2024-02-13 12:42:24,869] Trial 0 finished with value: 0.14937977492809296 and parameters: {'learning_rate': 0.007078668283306361, 'dropout_rate': 0.3940963456645584}. Best is trial 0 with value: 0.14937977492809296.\n",
      "[I 2024-02-13 12:42:32,493] Trial 1 finished with value: 0.149396613240242 and parameters: {'learning_rate': 0.0009315911954880989, 'dropout_rate': 0.38753918326618286}. Best is trial 0 with value: 0.14937977492809296.\n",
      "[I 2024-02-13 12:42:39,696] Trial 2 finished with value: 0.152899369597435 and parameters: {'learning_rate': 0.00021263151900675065, 'dropout_rate': 0.2135114356294884}. Best is trial 0 with value: 0.14937977492809296.\n",
      "[I 2024-02-13 12:42:46,615] Trial 3 finished with value: 0.1489623337984085 and parameters: {'learning_rate': 0.007633565920094789, 'dropout_rate': 0.17969968860646524}. Best is trial 3 with value: 0.1489623337984085.\n",
      "[I 2024-02-13 12:42:54,645] Trial 4 finished with value: 0.15635395050048828 and parameters: {'learning_rate': 0.0001706488574053257, 'dropout_rate': 0.4824831049025564}. Best is trial 3 with value: 0.1489623337984085.\n",
      "[I 2024-02-13 12:42:58,734] Trial 5 finished with value: 0.1482536941766739 and parameters: {'learning_rate': 0.006625361588434237, 'dropout_rate': 0.29961161187822294}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:04,004] Trial 6 finished with value: 0.14889000356197357 and parameters: {'learning_rate': 0.0008560989115260709, 'dropout_rate': 0.11915166643218172}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:11,481] Trial 7 finished with value: 0.1527947634458542 and parameters: {'learning_rate': 0.0002476759579924642, 'dropout_rate': 0.2313215551920107}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:18,548] Trial 8 finished with value: 0.15291671454906464 and parameters: {'learning_rate': 0.0002790991841344637, 'dropout_rate': 0.3993689866077468}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:25,488] Trial 9 finished with value: 0.14971955120563507 and parameters: {'learning_rate': 0.0008259104457726034, 'dropout_rate': 0.3549953922086785}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:32,675] Trial 10 finished with value: 0.14848187565803528 and parameters: {'learning_rate': 0.003816904888462991, 'dropout_rate': 0.2876790890632106}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:40,000] Trial 11 finished with value: 0.14898675680160522 and parameters: {'learning_rate': 0.003778546926201395, 'dropout_rate': 0.28641755881928754}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:44,141] Trial 12 finished with value: 0.1484948843717575 and parameters: {'learning_rate': 0.0027984235769114687, 'dropout_rate': 0.28529112116607164}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:47,857] Trial 13 finished with value: 0.1493273377418518 and parameters: {'learning_rate': 0.009989342721300596, 'dropout_rate': 0.32140738114933376}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:51,610] Trial 14 finished with value: 0.14879679679870605 and parameters: {'learning_rate': 0.002813904216425149, 'dropout_rate': 0.2606373753440873}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:55,439] Trial 15 finished with value: 0.14924128353595734 and parameters: {'learning_rate': 0.004559208599082612, 'dropout_rate': 0.31324473442869183}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:43:59,072] Trial 16 finished with value: 0.14861038327217102 and parameters: {'learning_rate': 0.0019112583646953567, 'dropout_rate': 0.24597461684728655}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:44:02,875] Trial 17 finished with value: 0.14893972873687744 and parameters: {'learning_rate': 0.001677999919508087, 'dropout_rate': 0.3293393669244806}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:44:07,242] Trial 18 finished with value: 0.14843247830867767 and parameters: {'learning_rate': 0.004752548223659395, 'dropout_rate': 0.1870593383149438}. Best is trial 5 with value: 0.1482536941766739.\n",
      "[I 2024-02-13 12:44:11,152] Trial 19 finished with value: 0.14880603551864624 and parameters: {'learning_rate': 0.005605454614414128, 'dropout_rate': 0.1765952149729588}. Best is trial 5 with value: 0.1482536941766739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.006625361588434237, 'dropout_rate': 0.29961161187822294}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    model = build_model(X_train.shape[1], y_train.shape[1], learning_rate, dropout_rate)\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    history = model.fit(X_train_split, y_train_split, epochs=30, validation_data=(X_val_split, y_val_split), verbose=0, batch_size=32)\n",
    "    \n",
    "    # 获取验证集上的最佳损失\n",
    "    best_loss = min(history.history['val_loss'])\n",
    "    return best_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20) # 根据需要调整试验次数\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "print('Best parameters:', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 1s - loss: 0.2441 - accuracy: 0.2502 - val_loss: 0.1616 - val_accuracy: 0.3646 - 1s/epoch - 28ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.1609 - accuracy: 0.3536 - val_loss: 0.1551 - val_accuracy: 0.3194 - 78ms/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.1561 - accuracy: 0.3145 - val_loss: 0.1550 - val_accuracy: 0.3854 - 85ms/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.1546 - accuracy: 0.3180 - val_loss: 0.1547 - val_accuracy: 0.3819 - 84ms/epoch - 2ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.1520 - accuracy: 0.3458 - val_loss: 0.1533 - val_accuracy: 0.3611 - 83ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.1512 - accuracy: 0.3510 - val_loss: 0.1521 - val_accuracy: 0.3125 - 85ms/epoch - 2ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.1499 - accuracy: 0.3354 - val_loss: 0.1526 - val_accuracy: 0.3021 - 84ms/epoch - 2ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.1499 - accuracy: 0.3301 - val_loss: 0.1525 - val_accuracy: 0.3576 - 104ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.1493 - accuracy: 0.3336 - val_loss: 0.1522 - val_accuracy: 0.3299 - 99ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.1489 - accuracy: 0.3406 - val_loss: 0.1506 - val_accuracy: 0.3021 - 98ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.1487 - accuracy: 0.3449 - val_loss: 0.1523 - val_accuracy: 0.2639 - 80ms/epoch - 2ms/step\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.1473 - accuracy: 0.3301 - val_loss: 0.1529 - val_accuracy: 0.2604 - 109ms/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.1473 - accuracy: 0.3136 - val_loss: 0.1514 - val_accuracy: 0.3264 - 112ms/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.1474 - accuracy: 0.3640 - val_loss: 0.1511 - val_accuracy: 0.3403 - 93ms/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.1456 - accuracy: 0.3371 - val_loss: 0.1510 - val_accuracy: 0.3403 - 85ms/epoch - 2ms/step\n",
      "Epoch 16/100\n",
      "36/36 - 0s - loss: 0.1460 - accuracy: 0.3519 - val_loss: 0.1499 - val_accuracy: 0.3229 - 84ms/epoch - 2ms/step\n",
      "Epoch 17/100\n",
      "36/36 - 0s - loss: 0.1450 - accuracy: 0.3458 - val_loss: 0.1511 - val_accuracy: 0.2951 - 99ms/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "36/36 - 0s - loss: 0.1455 - accuracy: 0.3319 - val_loss: 0.1497 - val_accuracy: 0.3194 - 98ms/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "36/36 - 0s - loss: 0.1453 - accuracy: 0.3484 - val_loss: 0.1499 - val_accuracy: 0.2986 - 101ms/epoch - 3ms/step\n",
      "Epoch 20/100\n",
      "36/36 - 0s - loss: 0.1449 - accuracy: 0.3467 - val_loss: 0.1491 - val_accuracy: 0.3229 - 100ms/epoch - 3ms/step\n",
      "Epoch 21/100\n",
      "36/36 - 0s - loss: 0.1457 - accuracy: 0.3475 - val_loss: 0.1485 - val_accuracy: 0.3021 - 132ms/epoch - 4ms/step\n",
      "Epoch 22/100\n",
      "36/36 - 0s - loss: 0.1457 - accuracy: 0.3223 - val_loss: 0.1503 - val_accuracy: 0.3611 - 98ms/epoch - 3ms/step\n",
      "Epoch 23/100\n",
      "36/36 - 0s - loss: 0.1453 - accuracy: 0.3467 - val_loss: 0.1514 - val_accuracy: 0.3507 - 85ms/epoch - 2ms/step\n",
      "Epoch 24/100\n",
      "36/36 - 0s - loss: 0.1440 - accuracy: 0.3371 - val_loss: 0.1486 - val_accuracy: 0.3507 - 83ms/epoch - 2ms/step\n",
      "Epoch 25/100\n",
      "36/36 - 0s - loss: 0.1443 - accuracy: 0.3510 - val_loss: 0.1501 - val_accuracy: 0.2847 - 83ms/epoch - 2ms/step\n",
      "Epoch 26/100\n",
      "36/36 - 0s - loss: 0.1444 - accuracy: 0.3440 - val_loss: 0.1488 - val_accuracy: 0.3368 - 99ms/epoch - 3ms/step\n",
      "Epoch 27/100\n",
      "36/36 - 0s - loss: 0.1442 - accuracy: 0.3536 - val_loss: 0.1501 - val_accuracy: 0.3438 - 88ms/epoch - 2ms/step\n",
      "Epoch 28/100\n",
      "36/36 - 0s - loss: 0.1440 - accuracy: 0.3579 - val_loss: 0.1488 - val_accuracy: 0.3264 - 97ms/epoch - 3ms/step\n",
      "Epoch 29/100\n",
      "36/36 - 0s - loss: 0.1436 - accuracy: 0.3267 - val_loss: 0.1490 - val_accuracy: 0.3611 - 102ms/epoch - 3ms/step\n",
      "Epoch 30/100\n",
      "36/36 - 0s - loss: 0.1433 - accuracy: 0.3354 - val_loss: 0.1503 - val_accuracy: 0.3472 - 86ms/epoch - 2ms/step\n",
      "Epoch 31/100\n",
      "36/36 - 0s - loss: 0.1437 - accuracy: 0.3536 - val_loss: 0.1494 - val_accuracy: 0.3333 - 100ms/epoch - 3ms/step\n",
      "Epoch 32/100\n",
      "36/36 - 0s - loss: 0.1432 - accuracy: 0.3362 - val_loss: 0.1497 - val_accuracy: 0.3715 - 85ms/epoch - 2ms/step\n",
      "Epoch 33/100\n",
      "36/36 - 0s - loss: 0.1434 - accuracy: 0.3440 - val_loss: 0.1491 - val_accuracy: 0.3438 - 95ms/epoch - 3ms/step\n",
      "Epoch 34/100\n",
      "36/36 - 0s - loss: 0.1439 - accuracy: 0.3510 - val_loss: 0.1492 - val_accuracy: 0.3403 - 100ms/epoch - 3ms/step\n",
      "Epoch 35/100\n",
      "36/36 - 0s - loss: 0.1438 - accuracy: 0.3275 - val_loss: 0.1490 - val_accuracy: 0.3264 - 83ms/epoch - 2ms/step\n",
      "Epoch 36/100\n",
      "36/36 - 0s - loss: 0.1436 - accuracy: 0.3449 - val_loss: 0.1486 - val_accuracy: 0.3021 - 84ms/epoch - 2ms/step\n",
      "Epoch 37/100\n",
      "36/36 - 0s - loss: 0.1434 - accuracy: 0.3380 - val_loss: 0.1496 - val_accuracy: 0.3646 - 100ms/epoch - 3ms/step\n",
      "Epoch 38/100\n",
      "36/36 - 0s - loss: 0.1435 - accuracy: 0.3449 - val_loss: 0.1487 - val_accuracy: 0.3438 - 83ms/epoch - 2ms/step\n",
      "Epoch 39/100\n",
      "36/36 - 0s - loss: 0.1428 - accuracy: 0.3588 - val_loss: 0.1496 - val_accuracy: 0.3090 - 147ms/epoch - 4ms/step\n",
      "Epoch 40/100\n",
      "36/36 - 0s - loss: 0.1436 - accuracy: 0.3406 - val_loss: 0.1496 - val_accuracy: 0.3125 - 81ms/epoch - 2ms/step\n",
      "Epoch 41/100\n",
      "36/36 - 0s - loss: 0.1426 - accuracy: 0.3527 - val_loss: 0.1502 - val_accuracy: 0.3229 - 109ms/epoch - 3ms/step\n",
      "Epoch 42/100\n",
      "36/36 - 0s - loss: 0.1424 - accuracy: 0.3623 - val_loss: 0.1496 - val_accuracy: 0.3229 - 87ms/epoch - 2ms/step\n",
      "Epoch 43/100\n",
      "36/36 - 0s - loss: 0.1428 - accuracy: 0.3423 - val_loss: 0.1496 - val_accuracy: 0.3229 - 85ms/epoch - 2ms/step\n",
      "Epoch 44/100\n",
      "36/36 - 0s - loss: 0.1422 - accuracy: 0.3380 - val_loss: 0.1486 - val_accuracy: 0.3403 - 103ms/epoch - 3ms/step\n",
      "Epoch 45/100\n",
      "36/36 - 0s - loss: 0.1428 - accuracy: 0.3632 - val_loss: 0.1501 - val_accuracy: 0.3264 - 121ms/epoch - 3ms/step\n",
      "Epoch 46/100\n",
      "36/36 - 0s - loss: 0.1430 - accuracy: 0.3406 - val_loss: 0.1486 - val_accuracy: 0.3715 - 104ms/epoch - 3ms/step\n",
      "Epoch 47/100\n",
      "36/36 - 0s - loss: 0.1430 - accuracy: 0.3345 - val_loss: 0.1487 - val_accuracy: 0.3021 - 90ms/epoch - 3ms/step\n",
      "Epoch 48/100\n",
      "36/36 - 0s - loss: 0.1431 - accuracy: 0.3632 - val_loss: 0.1485 - val_accuracy: 0.3125 - 100ms/epoch - 3ms/step\n",
      "Epoch 49/100\n",
      "36/36 - 0s - loss: 0.1432 - accuracy: 0.3467 - val_loss: 0.1489 - val_accuracy: 0.3229 - 96ms/epoch - 3ms/step\n",
      "Epoch 50/100\n",
      "36/36 - 0s - loss: 0.1427 - accuracy: 0.3388 - val_loss: 0.1487 - val_accuracy: 0.3229 - 102ms/epoch - 3ms/step\n",
      "Epoch 51/100\n",
      "36/36 - 0s - loss: 0.1430 - accuracy: 0.3328 - val_loss: 0.1484 - val_accuracy: 0.3125 - 96ms/epoch - 3ms/step\n",
      "Epoch 52/100\n",
      "36/36 - 0s - loss: 0.1426 - accuracy: 0.3414 - val_loss: 0.1490 - val_accuracy: 0.3056 - 104ms/epoch - 3ms/step\n",
      "Epoch 53/100\n",
      "36/36 - 0s - loss: 0.1426 - accuracy: 0.3397 - val_loss: 0.1485 - val_accuracy: 0.3438 - 78ms/epoch - 2ms/step\n",
      "Epoch 54/100\n",
      "36/36 - 0s - loss: 0.1420 - accuracy: 0.3519 - val_loss: 0.1484 - val_accuracy: 0.3333 - 85ms/epoch - 2ms/step\n",
      "Epoch 55/100\n",
      "36/36 - 0s - loss: 0.1421 - accuracy: 0.3501 - val_loss: 0.1482 - val_accuracy: 0.3333 - 84ms/epoch - 2ms/step\n",
      "Epoch 56/100\n",
      "36/36 - 0s - loss: 0.1425 - accuracy: 0.3545 - val_loss: 0.1488 - val_accuracy: 0.2847 - 99ms/epoch - 3ms/step\n",
      "Epoch 57/100\n",
      "36/36 - 0s - loss: 0.1418 - accuracy: 0.3215 - val_loss: 0.1483 - val_accuracy: 0.3507 - 85ms/epoch - 2ms/step\n",
      "Epoch 58/100\n",
      "36/36 - 0s - loss: 0.1416 - accuracy: 0.3527 - val_loss: 0.1490 - val_accuracy: 0.2778 - 100ms/epoch - 3ms/step\n",
      "Epoch 59/100\n",
      "36/36 - 0s - loss: 0.1416 - accuracy: 0.3336 - val_loss: 0.1493 - val_accuracy: 0.3125 - 97ms/epoch - 3ms/step\n",
      "Epoch 60/100\n",
      "36/36 - 0s - loss: 0.1424 - accuracy: 0.3649 - val_loss: 0.1489 - val_accuracy: 0.3229 - 86ms/epoch - 2ms/step\n",
      "Epoch 61/100\n",
      "36/36 - 0s - loss: 0.1422 - accuracy: 0.3597 - val_loss: 0.1486 - val_accuracy: 0.3681 - 112ms/epoch - 3ms/step\n",
      "Epoch 62/100\n",
      "36/36 - 0s - loss: 0.1419 - accuracy: 0.3536 - val_loss: 0.1481 - val_accuracy: 0.3576 - 100ms/epoch - 3ms/step\n",
      "Epoch 63/100\n",
      "36/36 - 0s - loss: 0.1422 - accuracy: 0.3484 - val_loss: 0.1486 - val_accuracy: 0.2812 - 121ms/epoch - 3ms/step\n",
      "Epoch 64/100\n",
      "36/36 - 0s - loss: 0.1418 - accuracy: 0.3215 - val_loss: 0.1495 - val_accuracy: 0.2917 - 101ms/epoch - 3ms/step\n",
      "Epoch 65/100\n",
      "36/36 - 0s - loss: 0.1415 - accuracy: 0.3797 - val_loss: 0.1485 - val_accuracy: 0.3125 - 100ms/epoch - 3ms/step\n",
      "Epoch 66/100\n",
      "36/36 - 0s - loss: 0.1412 - accuracy: 0.3249 - val_loss: 0.1486 - val_accuracy: 0.3368 - 101ms/epoch - 3ms/step\n",
      "Epoch 67/100\n",
      "36/36 - 0s - loss: 0.1420 - accuracy: 0.3484 - val_loss: 0.1489 - val_accuracy: 0.3299 - 99ms/epoch - 3ms/step\n",
      "Epoch 68/100\n",
      "36/36 - 0s - loss: 0.1419 - accuracy: 0.3293 - val_loss: 0.1489 - val_accuracy: 0.3229 - 101ms/epoch - 3ms/step\n",
      "Epoch 69/100\n",
      "36/36 - 0s - loss: 0.1413 - accuracy: 0.3606 - val_loss: 0.1490 - val_accuracy: 0.3299 - 100ms/epoch - 3ms/step\n",
      "Epoch 70/100\n",
      "36/36 - 0s - loss: 0.1414 - accuracy: 0.3249 - val_loss: 0.1485 - val_accuracy: 0.3507 - 100ms/epoch - 3ms/step\n",
      "Epoch 71/100\n",
      "36/36 - 0s - loss: 0.1413 - accuracy: 0.3745 - val_loss: 0.1491 - val_accuracy: 0.3333 - 99ms/epoch - 3ms/step\n",
      "Epoch 72/100\n",
      "36/36 - 0s - loss: 0.1411 - accuracy: 0.3449 - val_loss: 0.1489 - val_accuracy: 0.3264 - 101ms/epoch - 3ms/step\n",
      "Epoch 73/100\n",
      "36/36 - 0s - loss: 0.1414 - accuracy: 0.3293 - val_loss: 0.1488 - val_accuracy: 0.3125 - 190ms/epoch - 5ms/step\n",
      "Epoch 74/100\n",
      "36/36 - 0s - loss: 0.1412 - accuracy: 0.3510 - val_loss: 0.1492 - val_accuracy: 0.3125 - 111ms/epoch - 3ms/step\n",
      "Epoch 75/100\n",
      "36/36 - 0s - loss: 0.1415 - accuracy: 0.3388 - val_loss: 0.1485 - val_accuracy: 0.3507 - 104ms/epoch - 3ms/step\n",
      "Epoch 76/100\n",
      "36/36 - 0s - loss: 0.1419 - accuracy: 0.3328 - val_loss: 0.1495 - val_accuracy: 0.3715 - 98ms/epoch - 3ms/step\n",
      "Epoch 77/100\n",
      "36/36 - 0s - loss: 0.1418 - accuracy: 0.3519 - val_loss: 0.1486 - val_accuracy: 0.3125 - 99ms/epoch - 3ms/step\n",
      "Epoch 78/100\n",
      "36/36 - 0s - loss: 0.1414 - accuracy: 0.3640 - val_loss: 0.1489 - val_accuracy: 0.3438 - 103ms/epoch - 3ms/step\n",
      "Epoch 79/100\n",
      "36/36 - 0s - loss: 0.1415 - accuracy: 0.3293 - val_loss: 0.1492 - val_accuracy: 0.3576 - 112ms/epoch - 3ms/step\n",
      "Epoch 80/100\n",
      "36/36 - 0s - loss: 0.1417 - accuracy: 0.3545 - val_loss: 0.1485 - val_accuracy: 0.3333 - 100ms/epoch - 3ms/step\n",
      "Epoch 81/100\n",
      "36/36 - 0s - loss: 0.1408 - accuracy: 0.3310 - val_loss: 0.1502 - val_accuracy: 0.3507 - 100ms/epoch - 3ms/step\n",
      "Epoch 82/100\n",
      "36/36 - 0s - loss: 0.1413 - accuracy: 0.3684 - val_loss: 0.1496 - val_accuracy: 0.3125 - 100ms/epoch - 3ms/step\n",
      "Epoch 83/100\n",
      "36/36 - 0s - loss: 0.1413 - accuracy: 0.3345 - val_loss: 0.1485 - val_accuracy: 0.2535 - 88ms/epoch - 2ms/step\n",
      "Epoch 84/100\n",
      "36/36 - 0s - loss: 0.1410 - accuracy: 0.3336 - val_loss: 0.1480 - val_accuracy: 0.3299 - 97ms/epoch - 3ms/step\n",
      "Epoch 85/100\n",
      "36/36 - 0s - loss: 0.1409 - accuracy: 0.3519 - val_loss: 0.1486 - val_accuracy: 0.3333 - 101ms/epoch - 3ms/step\n",
      "Epoch 86/100\n",
      "36/36 - 0s - loss: 0.1414 - accuracy: 0.3467 - val_loss: 0.1488 - val_accuracy: 0.3542 - 87ms/epoch - 2ms/step\n",
      "Epoch 87/100\n",
      "36/36 - 0s - loss: 0.1416 - accuracy: 0.3666 - val_loss: 0.1496 - val_accuracy: 0.3403 - 98ms/epoch - 3ms/step\n",
      "Epoch 88/100\n",
      "36/36 - 0s - loss: 0.1416 - accuracy: 0.3328 - val_loss: 0.1488 - val_accuracy: 0.3229 - 101ms/epoch - 3ms/step\n",
      "Epoch 89/100\n",
      "36/36 - 0s - loss: 0.1405 - accuracy: 0.3241 - val_loss: 0.1486 - val_accuracy: 0.3507 - 106ms/epoch - 3ms/step\n",
      "Epoch 90/100\n",
      "36/36 - 0s - loss: 0.1408 - accuracy: 0.3380 - val_loss: 0.1489 - val_accuracy: 0.3160 - 110ms/epoch - 3ms/step\n",
      "Epoch 91/100\n",
      "36/36 - 0s - loss: 0.1403 - accuracy: 0.3527 - val_loss: 0.1486 - val_accuracy: 0.3403 - 100ms/epoch - 3ms/step\n",
      "Epoch 92/100\n",
      "36/36 - 0s - loss: 0.1411 - accuracy: 0.3345 - val_loss: 0.1486 - val_accuracy: 0.3438 - 100ms/epoch - 3ms/step\n",
      "Epoch 93/100\n",
      "36/36 - 0s - loss: 0.1409 - accuracy: 0.3545 - val_loss: 0.1488 - val_accuracy: 0.3090 - 100ms/epoch - 3ms/step\n",
      "Epoch 94/100\n",
      "36/36 - 0s - loss: 0.1412 - accuracy: 0.3579 - val_loss: 0.1486 - val_accuracy: 0.3333 - 126ms/epoch - 4ms/step\n",
      "Epoch 95/100\n",
      "36/36 - 0s - loss: 0.1409 - accuracy: 0.3336 - val_loss: 0.1482 - val_accuracy: 0.3403 - 89ms/epoch - 2ms/step\n",
      "Epoch 96/100\n",
      "36/36 - 0s - loss: 0.1399 - accuracy: 0.3493 - val_loss: 0.1484 - val_accuracy: 0.3090 - 99ms/epoch - 3ms/step\n",
      "Epoch 97/100\n",
      "36/36 - 0s - loss: 0.1405 - accuracy: 0.3380 - val_loss: 0.1480 - val_accuracy: 0.3090 - 99ms/epoch - 3ms/step\n",
      "Epoch 98/100\n",
      "36/36 - 0s - loss: 0.1408 - accuracy: 0.3510 - val_loss: 0.1485 - val_accuracy: 0.2951 - 118ms/epoch - 3ms/step\n",
      "Epoch 99/100\n",
      "36/36 - 0s - loss: 0.1410 - accuracy: 0.3197 - val_loss: 0.1481 - val_accuracy: 0.3333 - 92ms/epoch - 3ms/step\n",
      "Epoch 100/100\n",
      "36/36 - 0s - loss: 0.1406 - accuracy: 0.3354 - val_loss: 0.1486 - val_accuracy: 0.3090 - 98ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model(X_train.shape[1], y_train.shape[1], best_params['learning_rate'], best_params['dropout_rate'])\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 0s - loss: 0.1432 - accuracy: 0.3389 - 156ms/epoch - 13ms/step\n",
      "Test accuracy: 0.33888888359069824, Test loss: 0.14319536089897156\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step\n",
      "标签 0 的混淆矩阵:\n",
      "[[356   0]\n",
      " [  4   0]]\n",
      "\n",
      "标签 1 的混淆矩阵:\n",
      "[[195  15]\n",
      " [ 44 106]]\n",
      "\n",
      "标签 2 的混淆矩阵:\n",
      "[[310   0]\n",
      " [ 50   0]]\n",
      "\n",
      "标签 3 的混淆矩阵:\n",
      "[[350   0]\n",
      " [ 10   0]]\n",
      "\n",
      "标签 4 的混淆矩阵:\n",
      "[[311   7]\n",
      " [ 31  11]]\n",
      "\n",
      "标签 5 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 6 的混淆矩阵:\n",
      "[[348   0]\n",
      " [ 12   0]]\n",
      "\n",
      "标签 7 的混淆矩阵:\n",
      "[[350   0]\n",
      " [ 10   0]]\n",
      "\n",
      "标签 8 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 9 的混淆矩阵:\n",
      "[[346   0]\n",
      " [ 14   0]]\n",
      "\n",
      "标签 10 的混淆矩阵:\n",
      "[[338   0]\n",
      " [ 22   0]]\n",
      "\n",
      "标签 11 的混淆矩阵:\n",
      "[[354   0]\n",
      " [  6   0]]\n",
      "\n",
      "标签 12 的混淆矩阵:\n",
      "[[358   0]\n",
      " [  2   0]]\n",
      "\n",
      "标签 13 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n",
      "标签 14 的混淆矩阵:\n",
      "[[345   0]\n",
      " [ 15   0]]\n",
      "\n",
      "标签 15 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n",
      "标签 16 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n",
      "标签 17 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 18 的混淆矩阵:\n",
      "[[340   0]\n",
      " [ 20   0]]\n",
      "\n",
      "标签 19 的混淆矩阵:\n",
      "[[322   0]\n",
      " [ 38   0]]\n",
      "\n",
      "标签 20 的混淆矩阵:\n",
      "[[328   0]\n",
      " [ 32   0]]\n",
      "\n",
      "标签 21 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 22 的混淆矩阵:\n",
      "[[335   0]\n",
      " [ 25   0]]\n",
      "\n",
      "标签 23 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n",
      "标签 24 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 25 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 26 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 27 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n",
      "标签 28 的混淆矩阵:\n",
      "[[278   3]\n",
      " [ 67  12]]\n",
      "\n",
      "标签 29 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 30 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 31 的混淆矩阵:\n",
      "[[357   0]\n",
      " [  3   0]]\n",
      "\n",
      "标签 32 的混淆矩阵:\n",
      "[[289  14]\n",
      " [ 45  12]]\n",
      "\n",
      "标签 33 的混淆矩阵:\n",
      "[[245  26]\n",
      " [ 57  32]]\n",
      "\n",
      "标签 34 的混淆矩阵:\n",
      "[[345   0]\n",
      " [ 15   0]]\n",
      "\n",
      "标签 35 的混淆矩阵:\n",
      "[[360   0]\n",
      " [  0   0]]\n",
      "\n",
      "标签 36 的混淆矩阵:\n",
      "[[340   0]\n",
      " [ 20   0]]\n",
      "\n",
      "标签 37 的混淆矩阵:\n",
      "[[345   0]\n",
      " [ 15   0]]\n",
      "\n",
      "标签 38 的混淆矩阵:\n",
      "[[224  18]\n",
      " [103  15]]\n",
      "\n",
      "标签 39 的混淆矩阵:\n",
      "[[349   0]\n",
      " [ 11   0]]\n",
      "\n",
      "标签 40 的混淆矩阵:\n",
      "[[358   0]\n",
      " [  2   0]]\n",
      "\n",
      "标签 41 的混淆矩阵:\n",
      "[[328   0]\n",
      " [ 32   0]]\n",
      "\n",
      "标签 42 的混淆矩阵:\n",
      "[[339   0]\n",
      " [ 21   0]]\n",
      "\n",
      "标签 43 的混淆矩阵:\n",
      "[[231  17]\n",
      " [ 96  16]]\n",
      "\n",
      "标签 44 的混淆矩阵:\n",
      "[[345   0]\n",
      " [ 15   0]]\n",
      "\n",
      "标签 45 的混淆矩阵:\n",
      "[[344   0]\n",
      " [ 16   0]]\n",
      "\n",
      "标签 46 的混淆矩阵:\n",
      "[[353   0]\n",
      " [  7   0]]\n",
      "\n",
      "标签 47 的混淆矩阵:\n",
      "[[350   0]\n",
      " [ 10   0]]\n",
      "\n",
      "标签 48 的混淆矩阵:\n",
      "[[359   0]\n",
      " [  1   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# 假设y_test是测试集上的真实标签，y_pred是模型预测的标签\n",
    "y_pred = model.predict(X_test) > 0.5  # 采用0.5作为分类阈值\n",
    "confusion_matrices = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 打印每个标签的混淆矩阵\n",
    "for i, matrix in enumerate(confusion_matrices):\n",
    "    print(f\"标签 {i} 的混淆矩阵:\\n{matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体混淆矩阵:\n",
      "[[16460   100]\n",
      " [  876   204]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 假设 y_test 是真实的标签矩阵，y_pred 是预测的标签矩阵，且它们都是二进制的（0或1）\n",
    "# 对于多标签问题，我们可以通过将所有标签的预测视为一个长向量来简化问题\n",
    "y_test_flattened = y_test.flatten()\n",
    "y_pred_flattened = y_pred.flatten()\n",
    "\n",
    "# 计算整体混淆矩阵\n",
    "conf_matrix = confusion_matrix(y_test_flattened, y_pred_flattened)\n",
    "\n",
    "print(\"整体混淆矩阵:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体精度: 0.013888888888888888\n",
      "Hoodie 的正确率: 0.9888888888888889\n",
      "Unknown 的正确率: 0.8361111111111111\n",
      "coat 的正确率: 0.8611111111111112\n",
      "denim jacket 的正确率: 0.9722222222222222\n",
      "jackets 的正确率: 0.8944444444444445\n",
      "jeans 的正确率: 1.0\n",
      "knitted coat 的正确率: 0.9666666666666667\n",
      "long down jacket 的正确率: 0.9722222222222222\n",
      "long-sleeved dress 的正确率: 1.0\n",
      "shirts 的正确率: 0.9611111111111111\n",
      "short down jacket 的正确率: 0.9388888888888889\n",
      "short trench coat 的正确率: 0.9833333333333333\n",
      "short woolen coat 的正确率: 0.9944444444444445\n",
      "shorts 的正确率: 0.9972222222222222\n",
      "suit jackets 的正确率: 0.9583333333333334\n",
      "sweater 的正确率: 0.9972222222222222\n",
      "t-shirts 的正确率: 0.9972222222222222\n",
      "vest 的正确率: 1.0\n",
      "windbreaker 的正确率: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 计算整体精度\n",
    "y_pred_binary = (y_pred > 0.5)  # 将模型的预测结果转换为二进制形式\n",
    "overall_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"整体精度: {overall_accuracy}\")\n",
    "\n",
    "# 分别计算每个标签的正确率\n",
    "for i, label in enumerate(encoder.categories_[0]):\n",
    "    label_accuracy = accuracy_score(y_test[:, i], y_pred_binary[:, i])\n",
    "    print(f\"{label} 的正确率: {label_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('C:/1作品/picture_myclothes/my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
